\newpage

\section{Введение отношения порядка на множестве параметров аппроксимирующих моделей}
\subsection{Описание задачи}
В данной работе предлагается метод введения отношения порядка на множестве параметров сложных параметрических моделей, таких как нейросеть. Рассматривается порядок, заданный при помощи ковариационной матрицы градиентов функции ошибки по параметрам модели~\cite{Mandt2017}. В работе~\cite{Chunyan2016} предложен итерационный метод для поиска ковариационной матрицы градиентов. Данный итерационный метод интегрируется в градиентный метод оптимизации Adam~\cite{Kingma2014}.

Множество параметров упорядочивается по возрастанию дисперсии: от параметра с минимальной дисперсией до параметра с максимальной дисперсией градиента функции ошибки по соответствующему параметру модели. Предполагается, что малая дисперсия градиента указывает на то, что соответствующий параметр можно зафиксировать.

Для задания порядка на множестве параметров при помощи ковариационной матрицы вводится предположение о том, что фиксация параметров происходит в момент, когда все параметры модели находятся в некоторой окрестности локального минимума функции ошибки. Данное условие накладывается для корректного использования итерационного метода поиска ковариационной матрицы градиентов.

Заданный порядок на множестве параметров модели используется для фиксации тех параметров модели, которые оказываются предстоящими с точки зрения заданного порядка. Сначала фиксируются те параметры, которые имеют минимальную дисперсию градиента в окрестности локального минимума функции ошибки.

Для анализа свойств предложенного метода задания порядка на множестве параметров проводился вычислительный эксперимент. В качестве моделей рассматривались модели различной структурной сложности: линейные модели, нейросетевые модели. Предложенный метод задания порядка сравнивается с методом, в котором порядок задан произвольным образом.
\subsection{Формальная постановка задачи}
Задана выборка:
\begin{equation}
\label{eq:st:1}
\begin{aligned}
\mathfrak{D} = \bigr\{\bigr(\textbf{x}_i, y_i\bigr)\bigr\}_{i=1}^{m}, \quad \textbf{x}_{i} \in \mathbb{X} = \mathbb{R}^{n}, \quad y_i \in \mathbb{Y},
\end{aligned}
\end{equation}
где $n$~---~размерность признакового пространства, $m$~---~число объектов в выборке. Пространство ответов~$\mathbb{Y} = \mathbb{R}$ в случае задачи регрессии и  $\mathbb{Y} = \{1,\cdots, K\}$ в случае задачи классификации, где~$K$~---~число классов.

Задано семейство моделей параметрических функций с наперед заданной структурой:
\begin{equation}
\label{eq:st:2}
\begin{aligned}
\mathfrak{F} &= \bigr\{f\bigr(\textbf{w}\bigr):\mathbb{X} \to \mathbb{Y}~| \textbf{w} \in \mathbb{R}^{p}\bigr\}, \\ 
\mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr) &= \textbf{W}_1\bm{\sigma}\bigr(\textbf{W}_2\bm{\sigma}\bigr(\cdots\bm{\sigma}\bigr(\textbf{W}_r\textbf{x}\bigr)\cdots\bigr)\bigr),\\
f_{\text{\text{cl}}}\bigr(\textbf{w}, \textbf{x}\bigr) &= \arg \max_{j \in \bigr\{1,\cdots, K\bigr\}} \text{softmax}\bigr(\mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr)\bigr)_{j}, \\ 
f_{\text{reg}}\bigr(\textbf{w}, \textbf{x}\bigr) & = \mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr), 
\end{aligned}
\end{equation}
где $p$~---~размерность пространства параметров,~$r$~---~число слоев нейросети,~$\textbf{w} = \text{vec}[\textbf{W}_1, \textbf{W}_2, \cdots, \textbf{W}_r]$, а $\bm{\sigma}$~---~функция активации. В случае задачи регрессии структура модели имеет вид~$f_{\text{\text{reg}}}$, а в случае классификации имеет вид~$f_{\text{\text{cl}}}$.
%В качестве $\tau$ рассматривается~$\tau\bigr(\textbf{x}\bigr)~=~\textbf{x}$ в случае задачи регрессии, в случае задачи многоклассовой классификации~$\bm{\sigma}\bigr(\textbf{x}\bigr)~=~\text{softmax}\bigr(\textbf{x}\bigr)$.
Задана функция потерь:
\begin{equation}
\label{eq:st:3}
\begin{aligned}
\mathcal{L}\bigr(\textbf{w}, \mathfrak{D}\bigr) &= \frac{1}{m}\sum_{i=1}^{m}l\bigr(\textbf{x}_{i}, y_i, \textbf{w}\bigr),\\
l_{\text{\text{reg}}}\bigr(\textbf{x}, y, \textbf{w}\bigr) &= \bigr(y - f\bigr(\textbf{w}, \textbf{x}\bigr)\bigr)^{2},\\
l_{\text{\text{cl}}}\bigr(\textbf{x}, y, \textbf{w}\bigr) &= -\sum_{j=1}^{K}\bigr([y = j]\ln\text{softmax}_j\bigr(\mathbf{h}\bigr(\textbf{w}, \textbf{x}\bigr)\bigr)\bigr),
\end{aligned}
\end{equation}
где $l_{\text{\text{reg}}}$~---~это функция ошибки на одном элементе для задачи регрессии, $l_{\text{\text{cl}}}$~---~для задачи классификации.
Оптимальный вектор параметров $\hat{\textbf{w}}$ получим минимизацией функции потерь:
\begin{equation}
\label{eq:st:0:1}
\begin{aligned}
\hat{\textbf{w}} = \arg \min_{\textbf{w}\in\mathbb{R}^{p}} \mathcal{L}\bigr(\textbf{w}, \mathfrak{D}\bigr).
\end{aligned}
\end{equation}

 \subsection{Задание отношение порядка на множестве параметров}
Для поиска оптимальных параметров модели используется градиентный метод оптимизации:
\begin{equation}
\label{eq:st:4}
\begin{aligned}
\textbf{w}_{t} = \textbf{w}_{t-1} + \Delta\textbf{w}\bigr(\textbf{g}_{S,t}, \textbf{w}_{t-1}, \textbf{w}_{t-2}, \cdots\bigr), \quad \textbf{g}_{S,t}=\frac{\partial \mathcal{L}\bigr(\textbf{w}_{t}, \textbf{X}_{S}, \textbf{Y}_{S}\bigr)}{\partial \textbf{w}},
\end{aligned}
\end{equation}
где $t$~---~номер итерации, $\textbf{g}_{S,t}$~---~значение градиента на подвыборке размера $S$, $\Delta\textbf{w}$~---~приращение вектора параметров.
 
 
Порядок на множестве параметров модели задается при помощи ковариационной матрицы~$\textbf{C}$ градиентов функции ошибки~$\mathcal{L}$ по параметрам модели~$\textbf{w}$. Для вычисления ковариационной матрицы $\textbf{C}$ используется итерационная формула~\cite{Chunyan2016}, которая вычисляется на каждой итерации~\eqref{eq:st:4} градиентного метода оптимизации параметров:
\begin{equation}
\label{eq:st:5}
\begin{aligned}
\textbf{C}_t = \bigr(1-\kappa_t\bigr)\textbf{C}_{t-1}+\kappa_t\bigr(\textbf{g}_{1,t}-\textbf{g}_{S,t}\bigr)\bigr(\textbf{g}_{1,t}-\textbf{g}_{S,t}\bigr)^{\mathsf{T}},
\end{aligned}
\end{equation}
 где $t$~---~номер итерации, $\textbf{g}_{S,t}$~---~значение градиента на подвыборке размера $S$, $\textbf{g}_{1,t}$~---~значение градиента на первом элементе подвыборки, $\kappa_t=\frac{1}{t}$~---~параметр сглаживания, $\textbf{C}_0$ инициализируются из равномерного распределения.
 
Пусть известно $t_0$~---~число итераций, после которого все параметры находятся в некоторой локальной окрестности минимума, тогда, как показано в работе~\cite{Chunyan2016}, матрица~$\textbf{C}_{t_0}$ аппроксимирует истинную ковариационную матрицу~$\textbf{C}$. Ковариационная матрица $\textbf{C}_{t_0}$ используется для упорядочения параметров модели $\textbf{w}_{t_0}$. 
 
Пусть $\mathcal{I}$~---~ упорядоченный вектор индексов $[1, 2, \cdots, p]$. Обозначим $\mathcal{I}_{\textbf{w}_{t_0}}$ вектор индексов, порядок которого задан при помощи ковариационной матрицы $\textbf{C}_{t_0}$. 
 
Например, если ковариационная матрица $\textbf{C}_{t_0}$  имеет вид
 $$
\begin{bmatrix}
0{,}3& 0 & 0\\
0& 0{,}2 & 0\\
0& 0 & 0{,}25\\
\end{bmatrix},
 $$
 то вектор индексов $\mathcal{I}_{\textbf{w}_{t_0}} = [3,1,2]$.
 

\subsection{Фиксация параметров}
Для фиксации параметров $\textbf{w}_{t_0}$ при помощи вектора индексов $\mathcal{I}_{\textbf{w}_{t_0}}$ используется бинарный вектор $\bm{\alpha}\bigr(k\bigr)$:
\begin{equation}
\label{eq:st:6}
\begin{aligned}
\alpha_i\bigr(k\bigr) = \begin{cases}
   1, &\text{если }\mathcal{I}_{\textbf{w}_{t_0}}[j] \leq k;\\
   0 &\text{иначе},
 \end{cases}
\end{aligned}
\end{equation}
 где $k$~---~число фиксирующих параметров.
 
 Учитывая~\eqref{eq:st:6}, уравнение~\eqref{eq:st:4} приводится к виду
 \begin{equation}
\label{eq:st:7}
\begin{aligned}
\textbf{w}_{t} = \textbf{w}_{t-1} + \bm{\alpha}\bigr(k\bigr)\cdot\Delta\textbf{w}\bigr(\textbf{g}_{S,t}, \textbf{w}_{t-1}, \textbf{w}_{t-2}, \cdots\bigr),
\end{aligned}
\end{equation}
где $t$~---~номер итерации, $\textbf{g}_{S,t}$~---~значение градиента на подвыборке размера $S$, $\Delta\textbf{w}$~---~приращение вектора параметров. После умножения на бинарный вектор $\bm\alpha$ часть параметров не оптимизируется, что приводит к фиксации параметров.
